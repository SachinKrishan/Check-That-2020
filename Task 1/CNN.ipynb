{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>claim</th>\n",
       "      <th>check_worthiness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235714275752267776</td>\n",
       "      <td>https://twitter.com/julialindau/status/1235714...</td>\n",
       "      <td>I just landed at JFK after reporting on #coron...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235256530728972290</td>\n",
       "      <td>https://twitter.com/stayfrea_/status/123525653...</td>\n",
       "      <td>ALERT‼️‼️‼️ The corona virus can be spread thr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235648554338791427</td>\n",
       "      <td>https://twitter.com/A6Asap/status/123564855433...</td>\n",
       "      <td>COVID-19 health advice⚠️ https://t.co/XsSAo52Smu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235674258858061825</td>\n",
       "      <td>https://twitter.com/DrDenaGrayson/status/12356...</td>\n",
       "      <td>⚠️Chinese doctors say autopsies of #coronaviru...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235663306246860800</td>\n",
       "      <td>https://twitter.com/NNUBonnie/status/123566330...</td>\n",
       "      <td>.@NationalNurses President Deborah Burger read...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id             tweet_id  \\\n",
       "1  covid-19  1235714275752267776   \n",
       "2  covid-19  1235256530728972290   \n",
       "3  covid-19  1235648554338791427   \n",
       "4  covid-19  1235674258858061825   \n",
       "5  covid-19  1235663306246860800   \n",
       "\n",
       "                                           tweet_url  \\\n",
       "1  https://twitter.com/julialindau/status/1235714...   \n",
       "2  https://twitter.com/stayfrea_/status/123525653...   \n",
       "3  https://twitter.com/A6Asap/status/123564855433...   \n",
       "4  https://twitter.com/DrDenaGrayson/status/12356...   \n",
       "5  https://twitter.com/NNUBonnie/status/123566330...   \n",
       "\n",
       "                                          tweet_text claim check_worthiness  \n",
       "1  I just landed at JFK after reporting on #coron...     1                1  \n",
       "2  ALERT‼️‼️‼️ The corona virus can be spread thr...     1                0  \n",
       "3   COVID-19 health advice⚠️ https://t.co/XsSAo52Smu     0                0  \n",
       "4  ⚠️Chinese doctors say autopsies of #coronaviru...     1                1  \n",
       "5  .@NationalNurses President Deborah Burger read...     0                0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "prefix = 'v2/'\n",
    "train_df = pd.read_csv(prefix + 'training_v2.tsv',  sep='\\t', header=None)\n",
    "eval_df = pd.read_csv(prefix + 'dev_v2.tsv', sep='\\t', header=None)\n",
    "\n",
    "train_df.columns = list(train_df.iloc[0])\n",
    "train_df = train_df[1:]\n",
    "eval_df.columns = list(eval_df.iloc[0])\n",
    "eval_df = eval_df[1:]\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672, 6)\n",
      "\n",
      "claim:\n",
      " 1    410\n",
      "0    262\n",
      "Name: claim, dtype: int64\n",
      "\n",
      "check_worthiness:\n",
      " 0    441\n",
      "1    231\n",
      "Name: check_worthiness, dtype: int64\n",
      "\n",
      " (150, 6)\n",
      "\n",
      "claim:\n",
      " 1    97\n",
      "0    53\n",
      "Name: claim, dtype: int64\n",
      "\n",
      "check_worthiness:\n",
      " 0    91\n",
      "1    59\n",
      "Name: check_worthiness, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print('\\nclaim:\\n',train_df.claim.value_counts())\n",
    "print('\\ncheck_worthiness:\\n',train_df.check_worthiness.value_counts())\n",
    "\n",
    "print('\\n',eval_df.shape)\n",
    "print('\\nclaim:\\n',eval_df.claim.value_counts())\n",
    "print('\\ncheck_worthiness:\\n',eval_df.check_worthiness.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>claim</th>\n",
       "      <th>check_worthiness</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235714275752267776</td>\n",
       "      <td>https://twitter.com/julialindau/status/1235714...</td>\n",
       "      <td>I just landed at JFK after reporting on #coron...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I just landed at JFK after reporting on corona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235256530728972290</td>\n",
       "      <td>https://twitter.com/stayfrea_/status/123525653...</td>\n",
       "      <td>ALERT‼️‼️‼️ The corona virus can be spread thr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ALERT‼️‼️‼️ The corona virus can be spread thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235648554338791427</td>\n",
       "      <td>https://twitter.com/A6Asap/status/123564855433...</td>\n",
       "      <td>COVID-19 health advice⚠️ https://t.co/XsSAo52Smu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COVID19 health advice⚠️ httpstcoXsSAo52Smu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235674258858061825</td>\n",
       "      <td>https://twitter.com/DrDenaGrayson/status/12356...</td>\n",
       "      <td>⚠️Chinese doctors say autopsies of #coronaviru...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>⚠️Chinese doctors say autopsies of coronavirus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235663306246860800</td>\n",
       "      <td>https://twitter.com/NNUBonnie/status/123566330...</td>\n",
       "      <td>.@NationalNurses President Deborah Burger read...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NationalNurses President Deborah Burger reads ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id             tweet_id  \\\n",
       "1  covid-19  1235714275752267776   \n",
       "2  covid-19  1235256530728972290   \n",
       "3  covid-19  1235648554338791427   \n",
       "4  covid-19  1235674258858061825   \n",
       "5  covid-19  1235663306246860800   \n",
       "\n",
       "                                           tweet_url  \\\n",
       "1  https://twitter.com/julialindau/status/1235714...   \n",
       "2  https://twitter.com/stayfrea_/status/123525653...   \n",
       "3  https://twitter.com/A6Asap/status/123564855433...   \n",
       "4  https://twitter.com/DrDenaGrayson/status/12356...   \n",
       "5  https://twitter.com/NNUBonnie/status/123566330...   \n",
       "\n",
       "                                          tweet_text claim check_worthiness  \\\n",
       "1  I just landed at JFK after reporting on #coron...     1                1   \n",
       "2  ALERT‼️‼️‼️ The corona virus can be spread thr...     1                0   \n",
       "3   COVID-19 health advice⚠️ https://t.co/XsSAo52Smu     0                0   \n",
       "4  ⚠️Chinese doctors say autopsies of #coronaviru...     1                1   \n",
       "5  .@NationalNurses President Deborah Burger read...     0                0   \n",
       "\n",
       "                                          text_clean  \n",
       "1  I just landed at JFK after reporting on corona...  \n",
       "2  ALERT‼️‼️‼️ The corona virus can be spread thr...  \n",
       "3         COVID19 health advice⚠️ httpstcoXsSAo52Smu  \n",
       "4  ⚠️Chinese doctors say autopsies of coronavirus...  \n",
       "5  NationalNurses President Deborah Burger reads ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_nopunct\n",
    "\n",
    "train_df['text_clean'] = train_df['tweet_text'].apply(lambda x: remove_punct(x))\n",
    "eval_df['text_clean'] = eval_df['tweet_text'].apply(lambda x: remove_punct(x))\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "train_tokens = [word_tokenize(sen) for sen in train_df.text_clean]\n",
    "eval_tokens = [word_tokenize(sen) for sen in eval_df.text_clean]\n",
    "\n",
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "train_lower_tokens = [lower_token(token) for token in train_tokens]\n",
    "eval_lower_tokens = [lower_token(token) for token in eval_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>claim</th>\n",
       "      <th>check_worthiness</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235714275752267776</td>\n",
       "      <td>https://twitter.com/julialindau/status/1235714...</td>\n",
       "      <td>I just landed at JFK after reporting on #coron...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I just landed at JFK after reporting on corona...</td>\n",
       "      <td>landed jfk reporting coronavirus milan lombard...</td>\n",
       "      <td>[landed, jfk, reporting, coronavirus, milan, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235256530728972290</td>\n",
       "      <td>https://twitter.com/stayfrea_/status/123525653...</td>\n",
       "      <td>ALERT‼️‼️‼️ The corona virus can be spread thr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ALERT‼️‼️‼️ The corona virus can be spread thr...</td>\n",
       "      <td>alert‼️‼️‼️ corona virus spread money money ho...</td>\n",
       "      <td>[alert‼️‼️‼️, corona, virus, spread, money, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235648554338791427</td>\n",
       "      <td>https://twitter.com/A6Asap/status/123564855433...</td>\n",
       "      <td>COVID-19 health advice⚠️ https://t.co/XsSAo52Smu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COVID19 health advice⚠️ httpstcoXsSAo52Smu</td>\n",
       "      <td>covid19 health advice⚠️ httpstcoxssao52smu</td>\n",
       "      <td>[covid19, health, advice⚠️, httpstcoxssao52smu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235674258858061825</td>\n",
       "      <td>https://twitter.com/DrDenaGrayson/status/12356...</td>\n",
       "      <td>⚠️Chinese doctors say autopsies of #coronaviru...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>⚠️Chinese doctors say autopsies of coronavirus...</td>\n",
       "      <td>⚠️chinese doctors say autopsies coronavirus vi...</td>\n",
       "      <td>[⚠️chinese, doctors, say, autopsies, coronavir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235663306246860800</td>\n",
       "      <td>https://twitter.com/NNUBonnie/status/123566330...</td>\n",
       "      <td>.@NationalNurses President Deborah Burger read...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NationalNurses President Deborah Burger reads ...</td>\n",
       "      <td>nationalnurses president deborah burger reads ...</td>\n",
       "      <td>[nationalnurses, president, deborah, burger, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id             tweet_id  \\\n",
       "1  covid-19  1235714275752267776   \n",
       "2  covid-19  1235256530728972290   \n",
       "3  covid-19  1235648554338791427   \n",
       "4  covid-19  1235674258858061825   \n",
       "5  covid-19  1235663306246860800   \n",
       "\n",
       "                                           tweet_url  \\\n",
       "1  https://twitter.com/julialindau/status/1235714...   \n",
       "2  https://twitter.com/stayfrea_/status/123525653...   \n",
       "3  https://twitter.com/A6Asap/status/123564855433...   \n",
       "4  https://twitter.com/DrDenaGrayson/status/12356...   \n",
       "5  https://twitter.com/NNUBonnie/status/123566330...   \n",
       "\n",
       "                                          tweet_text claim check_worthiness  \\\n",
       "1  I just landed at JFK after reporting on #coron...     1                1   \n",
       "2  ALERT‼️‼️‼️ The corona virus can be spread thr...     1                0   \n",
       "3   COVID-19 health advice⚠️ https://t.co/XsSAo52Smu     0                0   \n",
       "4  ⚠️Chinese doctors say autopsies of #coronaviru...     1                1   \n",
       "5  .@NationalNurses President Deborah Burger read...     0                0   \n",
       "\n",
       "                                          text_clean  \\\n",
       "1  I just landed at JFK after reporting on corona...   \n",
       "2  ALERT‼️‼️‼️ The corona virus can be spread thr...   \n",
       "3         COVID19 health advice⚠️ httpstcoXsSAo52Smu   \n",
       "4  ⚠️Chinese doctors say autopsies of coronavirus...   \n",
       "5  NationalNurses President Deborah Burger reads ...   \n",
       "\n",
       "                                          Text_Final  \\\n",
       "1  landed jfk reporting coronavirus milan lombard...   \n",
       "2  alert‼️‼️‼️ corona virus spread money money ho...   \n",
       "3         covid19 health advice⚠️ httpstcoxssao52smu   \n",
       "4  ⚠️chinese doctors say autopsies coronavirus vi...   \n",
       "5  nationalnurses president deborah burger reads ...   \n",
       "\n",
       "                                              tokens  \n",
       "1  [landed, jfk, reporting, coronavirus, milan, l...  \n",
       "2  [alert‼️‼️‼️, corona, virus, spread, money, mo...  \n",
       "3    [covid19, health, advice⚠️, httpstcoxssao52smu]  \n",
       "4  [⚠️chinese, doctors, say, autopsies, coronavir...  \n",
       "5  [nationalnurses, president, deborah, burger, r...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "def removeStopWords(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]\n",
    "\n",
    "train_filtered_words = [removeStopWords(sen) for sen in train_lower_tokens]\n",
    "train_df['Text_Final'] = [' '.join(sen) for sen in train_filtered_words]\n",
    "train_df['tokens'] = train_filtered_words\n",
    "\n",
    "eval_filtered_words = [removeStopWords(sen) for sen in eval_lower_tokens]\n",
    "eval_df['Text_Final'] = [' '.join(sen) for sen in eval_filtered_words]\n",
    "eval_df['tokens'] = eval_filtered_words\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_Pos</th>\n",
       "      <th>claim_Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>landed jfk reporting coronavirus milan lombard...</td>\n",
       "      <td>[landed, jfk, reporting, coronavirus, milan, l...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alert‼️‼️‼️ corona virus spread money money ho...</td>\n",
       "      <td>[alert‼️‼️‼️, corona, virus, spread, money, mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid19 health advice⚠️ httpstcoxssao52smu</td>\n",
       "      <td>[covid19, health, advice⚠️, httpstcoxssao52smu]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>⚠️chinese doctors say autopsies coronavirus vi...</td>\n",
       "      <td>[⚠️chinese, doctors, say, autopsies, coronavir...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nationalnurses president deborah burger reads ...</td>\n",
       "      <td>[nationalnurses, president, deborah, burger, r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text_Final  \\\n",
       "1  landed jfk reporting coronavirus milan lombard...   \n",
       "2  alert‼️‼️‼️ corona virus spread money money ho...   \n",
       "3         covid19 health advice⚠️ httpstcoxssao52smu   \n",
       "4  ⚠️chinese doctors say autopsies coronavirus vi...   \n",
       "5  nationalnurses president deborah burger reads ...   \n",
       "\n",
       "                                              tokens claim  claim_Pos  \\\n",
       "1  [landed, jfk, reporting, coronavirus, milan, l...     1          1   \n",
       "2  [alert‼️‼️‼️, corona, virus, spread, money, mo...     1          1   \n",
       "3    [covid19, health, advice⚠️, httpstcoxssao52smu]     0          0   \n",
       "4  [⚠️chinese, doctors, say, autopsies, coronavir...     1          1   \n",
       "5  [nationalnurses, president, deborah, burger, r...     0          0   \n",
       "\n",
       "   claim_Neg  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  \n",
       "5          1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for l in train_df.claim:\n",
    "    if int(l) == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif int(l) == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        \n",
    "train_df['claim_Pos']= pos\n",
    "train_df['claim_Neg']= neg\n",
    "\n",
    "train_claim_df = train_df[['Text_Final', 'tokens', 'claim', 'claim_Pos', 'claim_Neg']]\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "for l in eval_df.claim:\n",
    "    if int(l) == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif int(l) == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        \n",
    "eval_df['claim_Pos']= pos\n",
    "eval_df['claim_Neg']= neg\n",
    "\n",
    "eval_claim_df = eval_df[['Text_Final', 'tokens', 'claim', 'claim_Pos', 'claim_Neg']]\n",
    "\n",
    "eval_claim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>check_worthiness</th>\n",
       "      <th>worth_Pos</th>\n",
       "      <th>worth_Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>landed jfk reporting coronavirus milan lombard...</td>\n",
       "      <td>[landed, jfk, reporting, coronavirus, milan, l...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alert‼️‼️‼️ corona virus spread money money ho...</td>\n",
       "      <td>[alert‼️‼️‼️, corona, virus, spread, money, mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid19 health advice⚠️ httpstcoxssao52smu</td>\n",
       "      <td>[covid19, health, advice⚠️, httpstcoxssao52smu]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>⚠️chinese doctors say autopsies coronavirus vi...</td>\n",
       "      <td>[⚠️chinese, doctors, say, autopsies, coronavir...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nationalnurses president deborah burger reads ...</td>\n",
       "      <td>[nationalnurses, president, deborah, burger, r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text_Final  \\\n",
       "1  landed jfk reporting coronavirus milan lombard...   \n",
       "2  alert‼️‼️‼️ corona virus spread money money ho...   \n",
       "3         covid19 health advice⚠️ httpstcoxssao52smu   \n",
       "4  ⚠️chinese doctors say autopsies coronavirus vi...   \n",
       "5  nationalnurses president deborah burger reads ...   \n",
       "\n",
       "                                              tokens check_worthiness  \\\n",
       "1  [landed, jfk, reporting, coronavirus, milan, l...                1   \n",
       "2  [alert‼️‼️‼️, corona, virus, spread, money, mo...                0   \n",
       "3    [covid19, health, advice⚠️, httpstcoxssao52smu]                0   \n",
       "4  [⚠️chinese, doctors, say, autopsies, coronavir...                1   \n",
       "5  [nationalnurses, president, deborah, burger, r...                0   \n",
       "\n",
       "   worth_Pos  worth_Neg  \n",
       "1          1          0  \n",
       "2          0          1  \n",
       "3          0          1  \n",
       "4          1          0  \n",
       "5          0          1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for l in train_df.check_worthiness:\n",
    "    if int(l) == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif int(l) == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        \n",
    "train_df['worth_Pos']= pos\n",
    "train_df['worth_Neg']= neg\n",
    "\n",
    "train_worth_df = train_df[['Text_Final', 'tokens', 'check_worthiness', 'worth_Pos', 'worth_Neg']]\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "for l in eval_df.check_worthiness:\n",
    "    if int(l) == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif int(l) == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        \n",
    "eval_df['worth_Pos']= pos\n",
    "eval_df['worth_Neg']= neg\n",
    "\n",
    "eval_worth_df = eval_df[['Text_Final', 'tokens', 'check_worthiness', 'worth_Pos', 'worth_Neg']]\n",
    "\n",
    "eval_worth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13190 words total, with a vocabulary size of 4960\n",
      "Max sentence length is 52\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in train_worth_df[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in train_worth_df[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3097 words total, with a vocabulary size of 1673\n",
      "Max sentence length is 49\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in eval_worth_df[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in eval_worth_df[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "worth_df = pd.concat([eval_worth_df, train_worth_df])\n",
    "\n",
    "model = models.Word2Vec(sentences = worth_df['tokens'], size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "filename = 'gensim_word2vec_model.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, train_worth_df, generate_missing=True)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4960 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(train_worth_df[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(train_worth_df[\"Text_Final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4961, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(eval_worth_df[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['worth_Pos', 'worth_Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_worth_df[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 300)    1488300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 999, 200)     120200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 998, 200)     180200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 997, 200)     240200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 996, 200)     300200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 995, 200)     360200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000)         0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          128128      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,817,686\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 1,488,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 604 samples, validate on 68 samples\n",
      "Epoch 1/10\n",
      "604/604 [==============================] - 37s 61ms/step - loss: 0.3567 - acc: 0.8725 - val_loss: 0.7831 - val_acc: 0.5588\n",
      "Epoch 2/10\n",
      "604/604 [==============================] - 35s 59ms/step - loss: 0.2683 - acc: 0.8965 - val_loss: 0.7063 - val_acc: 0.6103\n",
      "Epoch 3/10\n",
      "604/604 [==============================] - 39s 64ms/step - loss: 0.1307 - acc: 0.9793 - val_loss: 0.8702 - val_acc: 0.6324\n",
      "Epoch 4/10\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 0.0674 - acc: 0.9942 - val_loss: 0.7820 - val_acc: 0.6397\n",
      "Epoch 5/10\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 0.0348 - acc: 0.9983 - val_loss: 0.7706 - val_acc: 0.6250\n",
      "Epoch 6/10\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.9332 - val_acc: 0.6471\n",
      "Epoch 7/10\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.9499 - val_acc: 0.6618\n",
      "Epoch 8/10\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 0.0091 - acc: 0.9992 - val_loss: 0.9869 - val_acc: 0.6691\n",
      "Epoch 9/10\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 1.0742 - val_acc: 0.6471\n",
      "Epoch 10/10\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 1.4746 - val_acc: 0.5809\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "150/150 [==============================] - 4s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.28962708e-02, 9.65939522e-01],\n",
       "       [2.17623115e-02, 9.86365318e-01],\n",
       "       [5.19275665e-04, 9.99599457e-01],\n",
       "       [5.35125136e-02, 9.49980259e-01],\n",
       "       [3.10888439e-01, 7.36606359e-01],\n",
       "       [6.75777316e-01, 3.91825020e-01],\n",
       "       [3.56486440e-03, 9.97589648e-01],\n",
       "       [7.41099211e-05, 9.99958277e-01],\n",
       "       [3.54892015e-03, 9.97552156e-01],\n",
       "       [4.70826030e-03, 9.96361136e-01],\n",
       "       [5.55048227e-01, 5.15073359e-01],\n",
       "       [1.93297118e-01, 8.23266447e-01],\n",
       "       [3.70458722e-01, 6.96191967e-01],\n",
       "       [8.00428391e-02, 9.28226948e-01],\n",
       "       [4.28771973e-03, 9.96693969e-01],\n",
       "       [8.86392236e-01, 1.68640614e-01],\n",
       "       [1.55294806e-01, 8.56307030e-01],\n",
       "       [2.10961938e-01, 7.92540193e-01],\n",
       "       [2.02807784e-02, 9.83283758e-01],\n",
       "       [1.89784467e-02, 9.80797708e-01],\n",
       "       [1.64558887e-01, 8.59206319e-01],\n",
       "       [2.55733728e-04, 9.99836683e-01],\n",
       "       [4.55227196e-02, 9.61600780e-01],\n",
       "       [1.12549940e-04, 9.99902666e-01],\n",
       "       [9.39151645e-03, 9.91067648e-01],\n",
       "       [1.51152283e-01, 8.67330313e-01],\n",
       "       [2.66337693e-02, 9.79688346e-01],\n",
       "       [9.59828138e-01, 4.85597253e-02],\n",
       "       [1.05840564e-02, 9.91727769e-01],\n",
       "       [8.43759000e-01, 2.23269880e-01],\n",
       "       [2.42197514e-03, 9.97331381e-01],\n",
       "       [1.68684125e-03, 9.98630881e-01],\n",
       "       [1.22951865e-01, 8.98515582e-01],\n",
       "       [3.16712260e-03, 9.97466385e-01],\n",
       "       [1.74533635e-01, 8.57944846e-01],\n",
       "       [9.32329357e-01, 6.80376887e-02],\n",
       "       [2.31325626e-04, 9.99831080e-01],\n",
       "       [2.21475363e-02, 9.85862732e-01],\n",
       "       [1.92139447e-02, 9.86166596e-01],\n",
       "       [6.03023171e-03, 9.95113432e-01],\n",
       "       [1.06494427e-02, 9.91530180e-01],\n",
       "       [5.15481532e-02, 9.55093443e-01],\n",
       "       [9.00151730e-02, 9.22670126e-01],\n",
       "       [2.95966864e-04, 9.99758720e-01],\n",
       "       [3.29979718e-01, 7.21020103e-01],\n",
       "       [2.74672348e-05, 9.99972939e-01],\n",
       "       [5.87110877e-01, 4.64978695e-01],\n",
       "       [2.33410001e-02, 9.83339250e-01],\n",
       "       [2.13003755e-02, 9.79255438e-01],\n",
       "       [1.24782324e-03, 9.98997331e-01],\n",
       "       [2.43601203e-03, 9.98294890e-01],\n",
       "       [9.74125803e-01, 3.22167277e-02],\n",
       "       [6.84867628e-05, 9.99951541e-01],\n",
       "       [3.18348408e-04, 9.99789417e-01],\n",
       "       [1.31067336e-02, 9.90607262e-01],\n",
       "       [6.29740059e-02, 9.51395333e-01],\n",
       "       [8.84059787e-01, 1.40491098e-01],\n",
       "       [1.27574801e-03, 9.99212384e-01],\n",
       "       [4.89027441e-01, 5.78782499e-01],\n",
       "       [1.98662281e-04, 9.99824405e-01],\n",
       "       [3.96549702e-04, 9.99693155e-01],\n",
       "       [7.01516867e-04, 9.99565542e-01],\n",
       "       [9.80079174e-04, 9.99104500e-01],\n",
       "       [5.82396984e-04, 9.99569058e-01],\n",
       "       [2.00241804e-03, 9.98639464e-01],\n",
       "       [1.00172371e-01, 8.95026565e-01],\n",
       "       [9.78061557e-03, 9.92103517e-01],\n",
       "       [1.78990364e-02, 9.90270972e-01],\n",
       "       [1.46736205e-02, 9.87757862e-01],\n",
       "       [1.21181309e-01, 8.87405872e-01],\n",
       "       [2.08720267e-02, 9.75023150e-01],\n",
       "       [9.39737737e-01, 7.88876712e-02],\n",
       "       [3.88764471e-01, 6.69818997e-01],\n",
       "       [1.09911561e-02, 9.91844177e-01],\n",
       "       [7.84460068e-01, 2.58024931e-01],\n",
       "       [7.48233318e-01, 2.80587971e-01],\n",
       "       [2.08823264e-01, 8.03850055e-01],\n",
       "       [1.07146114e-01, 9.20213223e-01],\n",
       "       [2.14443505e-01, 8.22032750e-01],\n",
       "       [2.42736936e-03, 9.97705519e-01],\n",
       "       [6.96802497e-01, 3.86969000e-01],\n",
       "       [1.63926721e-01, 8.54993522e-01],\n",
       "       [5.60079813e-02, 9.45995808e-01],\n",
       "       [3.04262698e-01, 7.03486145e-01],\n",
       "       [6.23867810e-02, 9.47545648e-01],\n",
       "       [4.51505184e-04, 9.99613523e-01],\n",
       "       [6.53979182e-02, 9.53038394e-01],\n",
       "       [9.73424315e-03, 9.93597865e-01],\n",
       "       [1.18330806e-01, 8.86383891e-01],\n",
       "       [2.96859443e-01, 7.55830288e-01],\n",
       "       [8.17774832e-02, 9.29362178e-01],\n",
       "       [5.27485609e-02, 9.58936691e-01],\n",
       "       [6.68175817e-02, 9.45081353e-01],\n",
       "       [1.96790814e-01, 8.52531016e-01],\n",
       "       [2.36119032e-01, 7.93129683e-01],\n",
       "       [2.71659493e-02, 9.73428011e-01],\n",
       "       [8.38473439e-03, 9.94077563e-01],\n",
       "       [1.60904348e-01, 8.39197040e-01],\n",
       "       [3.43802750e-01, 7.39123106e-01],\n",
       "       [8.26981664e-03, 9.94425178e-01],\n",
       "       [1.10732943e-01, 8.90122235e-01],\n",
       "       [8.50061893e-01, 1.38605773e-01],\n",
       "       [2.11161375e-03, 9.98580277e-01],\n",
       "       [3.36366892e-03, 9.97494340e-01],\n",
       "       [2.10877359e-02, 9.82093036e-01],\n",
       "       [4.51931566e-01, 6.03469789e-01],\n",
       "       [1.10411346e-02, 9.91009295e-01],\n",
       "       [1.43253475e-01, 8.82048786e-01],\n",
       "       [5.86413443e-02, 9.45284963e-01],\n",
       "       [5.49651384e-02, 9.56348956e-01],\n",
       "       [7.52817392e-01, 2.67123342e-01],\n",
       "       [4.92453575e-04, 9.99624848e-01],\n",
       "       [8.97853613e-01, 1.40243322e-01],\n",
       "       [2.01951563e-02, 9.79169726e-01],\n",
       "       [2.14387923e-01, 8.21921051e-01],\n",
       "       [3.60396504e-03, 9.96815205e-01],\n",
       "       [5.43451309e-03, 9.94830012e-01],\n",
       "       [6.22844100e-02, 9.55354691e-01],\n",
       "       [3.17381024e-02, 9.76122499e-01],\n",
       "       [5.16274631e-01, 5.17388642e-01],\n",
       "       [1.59299612e-01, 8.84329975e-01],\n",
       "       [5.35733104e-02, 9.51114416e-01],\n",
       "       [5.50159514e-02, 9.45541561e-01],\n",
       "       [1.12812191e-01, 9.15953815e-01],\n",
       "       [9.23701048e-01, 7.92248547e-02],\n",
       "       [2.62182951e-03, 9.97954488e-01],\n",
       "       [1.71422958e-03, 9.98918176e-01],\n",
       "       [8.88256073e-01, 1.26640528e-01],\n",
       "       [1.09012723e-02, 9.91475284e-01],\n",
       "       [2.45963454e-01, 7.89043665e-01],\n",
       "       [1.74751878e-03, 9.98740792e-01],\n",
       "       [1.13449395e-02, 9.93525624e-01],\n",
       "       [5.79639673e-02, 9.53949332e-01],\n",
       "       [6.41526937e-01, 4.33055788e-01],\n",
       "       [7.75974989e-03, 9.94769990e-01],\n",
       "       [1.89006329e-04, 9.99858379e-01],\n",
       "       [1.33484602e-03, 9.99115527e-01],\n",
       "       [3.02692175e-01, 7.16202855e-01],\n",
       "       [1.89363956e-04, 9.99841332e-01],\n",
       "       [1.97726488e-03, 9.98676002e-01],\n",
       "       [9.69113111e-02, 9.18649316e-01],\n",
       "       [6.36282563e-01, 3.62325579e-01],\n",
       "       [3.18090975e-01, 6.90690756e-01],\n",
       "       [4.89434600e-03, 9.95308340e-01],\n",
       "       [8.04771483e-02, 9.47339892e-01],\n",
       "       [5.18790483e-02, 9.54339266e-01],\n",
       "       [1.27297640e-03, 9.98901486e-01],\n",
       "       [6.68039918e-03, 9.94811773e-01],\n",
       "       [3.82966804e-03, 9.97693360e-01],\n",
       "       [1.22712299e-01, 8.96152914e-01]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "    \n",
    "print(len(prediction_labels))\n",
    "prediction_labels\n",
    "\n",
    "#eval_worth_df.check_worthiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      False\n",
      "2      False\n",
      "3      False\n",
      "4      False\n",
      "5      False\n",
      "       ...  \n",
      "146    False\n",
      "147    False\n",
      "148    False\n",
      "149    False\n",
      "150    False\n",
      "Name: check_worthiness, Length: 150, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "sum(eval_worth_df.check_worthiness==prediction_labels)/len(prediction_labels)\n",
    "print(eval_worth_df.check_worthiness==prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    91\n",
       "1    59\n",
       "Name: check_worthiness, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_worth_df.check_worthiness.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.96      0.79        91\n",
      "           1       0.80      0.27      0.41        59\n",
      "\n",
      "    accuracy                           0.69       150\n",
      "   macro avg       0.73      0.61      0.60       150\n",
      "weighted avg       0.72      0.69      0.64       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "truth = []\n",
    "for l in eval_worth_df.check_worthiness:\n",
    "    truth.append(int(l))\n",
    "\n",
    "\n",
    "print(classification_report(truth, prediction_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
